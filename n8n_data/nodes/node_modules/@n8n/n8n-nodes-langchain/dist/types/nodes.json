[
{"displayName":"Agent","name":"@n8n/n8n-nodes-langchain.agent","icon":"fa:robot","group":["transform"],"version":1,"description":"Seamlessly coordinates LLMs & tools per user input","subtitle":"={{ {\tconversationalAgent: 'Conversational Agent', openAiFunctionsAgent: 'OpenAI Functions Agent', reactAgent: 'ReAct Agent', sqlAgent: 'SQL Agent' }[$parameter.agent] }}","defaults":{"name":"Agent","color":"#404040"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Agents"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/"}]}},"inputs":"={{ ((agent) => { function getInputs(agent) {\n    const getInputData = (inputs) => {\n        const displayNames = {\n            [\"ai_languageModel\"]: 'Model',\n            [\"ai_memory\"]: 'Memory',\n            [\"ai_tool\"]: 'Tool',\n            [\"ai_outputParser\"]: 'Output Parser',\n        };\n        return inputs.map(({ type, filter }) => {\n            const input = {\n                type,\n                displayName: type in displayNames ? displayNames[type] : undefined,\n                required: type === \"ai_languageModel\",\n                maxConnections: [\"ai_languageModel\", \"ai_memory\"].includes(type)\n                    ? 1\n                    : undefined,\n            };\n            if (filter) {\n                input.filter = filter;\n            }\n            return input;\n        });\n    };\n    let specialInputs = [];\n    if (agent === 'conversationalAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n                filter: {\n                    nodes: [\n                        '@n8n/n8n-nodes-langchain.lmChatAnthropic',\n                        '@n8n/n8n-nodes-langchain.lmChatOllama',\n                        '@n8n/n8n-nodes-langchain.lmChatOpenAi',\n                        '@n8n/n8n-nodes-langchain.lmChatGooglePalm',\n                    ],\n                },\n            },\n            {\n                type: \"ai_memory\",\n            },\n            {\n                type: \"ai_tool\",\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    else if (agent === 'openAiFunctionsAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n                filter: {\n                    nodes: ['@n8n/n8n-nodes-langchain.lmChatOpenAi'],\n                },\n            },\n            {\n                type: \"ai_memory\",\n            },\n            {\n                type: \"ai_tool\",\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    else if (agent === 'reActAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n            },\n            {\n                type: \"ai_tool\",\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    else if (agent === 'sqlAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n            },\n        ];\n    }\n    return [\"main\", ...getInputData(specialInputs)];\n}; return getInputs(agent) })($parameter.agent) }}","outputs":["main"],"credentials":[{"name":"mySql","required":true,"testedBy":"mysqlConnectionTest","displayOptions":{"show":{"agent":["sqlAgent"],"/dataSource":["mysql"]}}},{"name":"postgres","required":true,"displayOptions":{"show":{"agent":["sqlAgent"],"/dataSource":["postgres"]}}}],"properties":[{"displayName":"Agent","name":"agent","type":"options","noDataExpression":true,"options":[{"name":"Conversational Agent","value":"conversationalAgent","description":"Selects tools to accomplish its task and uses memory to recall previous conversations"},{"name":"OpenAI Functions Agent","value":"openAiFunctionsAgent","description":"Utilizes OpenAI's Function Calling feature to select the appropriate tool and arguments for execution"},{"name":"ReAct Agent","value":"reActAgent","description":"Strategically select tools to accomplish a given task"},{"name":"SQL Agent","value":"sqlAgent","description":"Answers questions about data in an SQL database"}],"default":"conversationalAgent"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["conversationalAgent"]}},"default":"={{ $json.input }}"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["conversationalAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"Human Message","name":"humanMessage","type":"string","default":"TOOLS\n------\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\n{tools}\n\n{format_instructions}\n\nUSER'S INPUT\n--------------------\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{{input}}","description":"The message that will provide the agent with a list of tools to use","typeOptions":{"rows":6}},{"displayName":"System Message","name":"systemMessage","type":"string","default":"Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.","description":"The message that will be sent to the agent before the conversation starts","typeOptions":{"rows":6}},{"displayName":"Max Iterations","name":"maxIterations","type":"number","default":10,"description":"The maximum number of iterations the agent will run before stopping"}]},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["openAiFunctionsAgent"]}},"default":"={{ $json.input }}"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["openAiFunctionsAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"System Message","name":"systemMessage","type":"string","default":"You are a helpful AI assistant.","description":"The message that will be sent to the agent before the conversation starts","typeOptions":{"rows":6}},{"displayName":"Max Iterations","name":"maxIterations","type":"number","default":10,"description":"The maximum number of iterations the agent will run before stopping"}]},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["reActAgent"]}},"default":"={{ $json.input }}"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["reActAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"Human Message Template","name":"humanMessageTemplate","type":"string","default":"{input}\n\n{agent_scratchpad}","description":"String to use directly as the human message template","typeOptions":{"rows":6}},{"displayName":"Prefix Message","name":"prefix","type":"string","default":"Answer the following questions as best you can. You have access to the following tools:","description":"String to put before the list of tools","typeOptions":{"rows":6}},{"displayName":"Suffix Message for Chat Model","name":"suffixChat","type":"string","default":"Begin! Reminder to always use the exact characters `Final Answer` when responding.","description":"String to put after the list of tools that will be used if chat model is used","typeOptions":{"rows":6}},{"displayName":"Suffix Message for Regular Model","name":"suffix","type":"string","default":"Begin!\n\n\tQuestion: {input}\n\tThought:{agent_scratchpad}","description":"String to put after the list of tools that will be used if regular model is used","typeOptions":{"rows":6}}]},{"displayName":"Data Source","name":"dataSource","type":"options","displayOptions":{"show":{"agent":["sqlAgent"]}},"default":"sqlite","description":"SQL database to connect to","options":[{"name":"MySQL","value":"mysql","description":"Connect to a MySQL database"},{"name":"Postgres","value":"postgres","description":"Connect to a Postgres database"},{"name":"SQLite","value":"sqlite","description":"Use SQLite by connecting a database file as binary input"}]},{"displayName":"Prompt","name":"input","type":"string","displayOptions":{"show":{"agent":["sqlAgent"]}},"default":"","required":true,"typeOptions":{"rows":5}},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["sqlAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"Ignored Tables","name":"ignoredTables","type":"string","default":"","description":"Comma-separated list of tables to ignore from the database. If empty, no tables are ignored."},{"displayName":"Include Sample Rows","name":"includedSampleRows","type":"number","description":"Number of sample rows to include in the prompt to the agent. It helps the agent to understand the schema of the database but it also increases the amount of tokens used.","default":3},{"displayName":"Included Tables","name":"includedTables","type":"string","default":"","description":"Comma-separated list of tables to include in the database. If empty, all tables are included."},{"displayName":"Prefix Prompt","name":"prefixPrompt","type":"string","default":"You are an agent designed to interact with an SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results using the LIMIT clause.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\nIf the question does not seem related to the database, just return \"I don't know\" as the answer.","description":"Prefix prompt to use for the agent","typeOptions":{"rows":10}},{"displayName":"Suffix Prompt","name":"suffixPrompt","type":"string","default":"Begin!\n\nQuestion: {input}\nThought: I should look at the tables in the database to see what I can query.\n{agent_scratchpad}","description":"Suffix prompt to use for the agent","typeOptions":{"rows":4}},{"displayName":"Top K","name":"topK","type":"number","default":10,"description":"Number of top results agent should return"}]}]},
{"displayName":"Summarization Chain","name":"@n8n/n8n-nodes-langchain.chainSummarization","icon":"fa:link","group":["transform"],"version":1,"description":"Transforms text into a concise summary","defaults":{"name":"Summarization Chain","color":"#909298"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Chains"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainsummarization/"}]}},"inputs":["main",{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true}],"outputs":["main"],"credentials":[],"properties":[{"displayName":"Specify the document to load in the document loader sub-node","name":"notice","type":"notice","default":""},{"displayName":"Type","name":"type","type":"options","description":"The type of summarization to run","default":"map_reduce","options":[{"name":"Map Reduce (Recommended)","value":"map_reduce","description":"Individually summarizes each document using an LLM (Map step), then combines these summaries into a global summary (Reduce step), with an optional compression step to ensure fit"},{"name":"Refine","value":"refine","description":"Iteratively updates its answer by looping over the documents and passing the current document along with the latest intermediate answer to an LLM, suitable for analyzing large document sets, albeit with more LLM calls"},{"name":"Stuff","value":"stuff","description":"Inserts all documents into a prompt, then passes it to an LLM for summarization, ideal for small document sets"}]},{"displayName":"Options","name":"options","type":"collection","default":{},"placeholder":"Add Option","options":[{"displayName":"Combine Map Prompt","name":"combineMapPrompt","type":"string","displayOptions":{"show":{"/type":["map_reduce"]}},"default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","typeOptions":{"rows":6}},{"displayName":"Prompt","name":"prompt","type":"string","default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","displayOptions":{"hide":{"/type":["refine"]}},"typeOptions":{"rows":6}},{"displayName":"Refine Prompt","name":"refinePrompt","type":"string","displayOptions":{"show":{"/type":["refine"]}},"default":"Your job is to produce a final summary\nWe have provided an existing summary up to a certain point: \"{existing_answer}\"\nWe have the opportunity to refine the existing summary\n(only if needed) with some more context below.\n------------\n\"{text}\"\n------------\n\nGiven the new context, refine the original summary\nIf the context isn't useful, return the original summary.\n\nREFINED SUMMARY:","typeOptions":{"rows":6}},{"displayName":"Refine Question Prompt","name":"refineQuestionPrompt","type":"string","displayOptions":{"show":{"/type":["refine"]}},"default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","typeOptions":{"rows":6}}]}]},
{"displayName":"Basic LLM Chain","name":"@n8n/n8n-nodes-langchain.chainLlm","icon":"fa:link","group":["transform"],"version":1,"description":"A simple chain to prompt a large language mode","defaults":{"name":"Basic LLM Chain","color":"#909298"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Chains"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/"}]}},"inputs":["main",{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Output Parser","type":"ai_outputParser","required":false}],"outputs":["main"],"credentials":[],"properties":[{"displayName":"Prompt","name":"prompt","type":"string","required":true,"default":"={{ $json.input }}"},{"displayName":"The options below to add prompts are only valid for chat models, they will be ignored for other models.","name":"notice","type":"notice","default":""},{"displayName":"Chat Messages","name":"messages","type":"fixedCollection","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add prompt","options":[{"name":"messageValues","displayName":"Prompt","values":[{"displayName":"Type Name or ID","name":"type","type":"options","options":[{"name":"AI","value":"AIMessagePromptTemplate"},{"name":"System","value":"SystemMessagePromptTemplate"},{"name":"User","value":"HumanMessagePromptTemplate"}],"default":"SystemMessagePromptTemplate"},{"displayName":"Message","name":"message","type":"string","required":true,"default":""}]}]}]},
{"displayName":"Retrieval Q&A Chain","name":"@n8n/n8n-nodes-langchain.chainRetrievalQa","icon":"fa:link","group":["transform"],"version":1,"description":"Retrieves answers to queries based on retrieved documents","defaults":{"name":"Retrieval Q&A Chain","color":"#909298"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Chains"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/"}]}},"inputs":["main",{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever","required":true}],"outputs":["main"],"credentials":[],"properties":[{"displayName":"Query","name":"query","type":"string","required":true,"default":"={{ $json.input }}"}]},
{"displayName":"LangChain Code","name":"@n8n/n8n-nodes-langchain.code","icon":"fa:code","group":["transform"],"version":1,"description":"LangChain Code Node","defaults":{"name":"LangChain Code"},"codex":{"categories":["AI"],"subcategories":{"AI":["Miscellaneous"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code/"}]}},"inputs":"={{ ((values) => { const connectorTypes = {\"ai_chain\":\"Chain\",\"ai_document\":\"Document\",\"ai_embedding\":\"Embedding\",\"ai_languageModel\":\"Language Model\",\"ai_memory\":\"Memory\",\"ai_outputParser\":\"Output Parser\",\"ai_textSplitter\":\"Text Splitter\",\"ai_tool\":\"Tool\",\"ai_vectorStore\":\"Vector Store\",\"main\":\"Main\"}; return values.map(value => { return { type: value.type, required: value.required, maxConnections: value.maxConnections === -1 ? undefined : value.maxConnections, displayName: connectorTypes[value.type] !== 'Main' ? connectorTypes[value.type] : undefined } } ) })($parameter.inputs.input) }}","outputs":"={{ ((values) => { const connectorTypes = {\"ai_chain\":\"Chain\",\"ai_document\":\"Document\",\"ai_embedding\":\"Embedding\",\"ai_languageModel\":\"Language Model\",\"ai_memory\":\"Memory\",\"ai_outputParser\":\"Output Parser\",\"ai_textSplitter\":\"Text Splitter\",\"ai_tool\":\"Tool\",\"ai_vectorStore\":\"Vector Store\",\"main\":\"Main\"}; return values.map(value => { return { type: value.type, displayName: connectorTypes[value.type] !== 'Main' ? connectorTypes[value.type] : undefined } } ) })($parameter.outputs.output) }}","properties":[{"displayName":"Code","name":"code","placeholder":"Add Code","type":"fixedCollection","noDataExpression":true,"default":{},"options":[{"name":"execute","displayName":"Execute","values":[{"displayName":"JavaScript - Execute","name":"code","type":"string","typeOptions":{"editor":"codeNodeEditor","editorLanguage":"javaScript"},"default":"const { PromptTemplate } = require('langchain/prompts');\n\nconst query = 'Tell me a joke';\nconst prompt = PromptTemplate.fromTemplate(query);\nconst llm = await this.getInputConnectionData('ai_languageModel', 0);\nlet chain = prompt.pipe(llm);\nconst output = await chain.invoke();\nreturn [ {json: { output } } ];","hint":"This code will only run and return data if a \"Main\" input & output got created.","noDataExpression":true}]},{"name":"supplyData","displayName":"Supply Data","values":[{"displayName":"JavaScript - Supply Data","name":"code","type":"string","typeOptions":{"editor":"codeNodeEditor","editorLanguage":"javaScript"},"default":"const { WikipediaQueryRun } = require('langchain/tools');\nreturn new WikipediaQueryRun();","hint":"This code will only run and return data if an output got created which is not \"Main\".","noDataExpression":true}]}]},{"displayName":"You can import LangChain and use all available functionality. Debug by using <code>console.log()</code> statements and viewing their output in the browser console.","name":"notice","type":"notice","default":""},{"displayName":"Inputs","name":"inputs","placeholder":"Add Input","type":"fixedCollection","noDataExpression":true,"typeOptions":{"multipleValues":true,"sortable":true},"description":"The input to add","default":{},"options":[{"name":"input","displayName":"Input","values":[{"displayName":"Type","name":"type","type":"options","options":[{"name":"Chain","value":"ai_chain"},{"name":"Document","value":"ai_document"},{"name":"Embedding","value":"ai_embedding"},{"name":"Language Model","value":"ai_languageModel"},{"name":"Memory","value":"ai_memory"},{"name":"Output Parser","value":"ai_outputParser"},{"name":"Text Splitter","value":"ai_textSplitter"},{"name":"Tool","value":"ai_tool"},{"name":"Vector Store","value":"ai_vectorStore"},{"name":"Main","value":"main"}],"noDataExpression":true,"default":"","required":true,"description":"The type of the input"},{"displayName":"Max Connections","name":"maxConnections","type":"number","noDataExpression":true,"default":-1,"required":true,"description":"How many nodes of this type are allowed to be connected. Set it to -1 for unlimited."},{"displayName":"Required","name":"required","type":"boolean","noDataExpression":true,"default":false,"required":true,"description":"Whether the input needs a connection"}]}]},{"displayName":"Outputs","name":"outputs","placeholder":"Add Output","type":"fixedCollection","noDataExpression":true,"typeOptions":{"multipleValues":true,"sortable":true},"description":"The output to add","default":{},"options":[{"name":"output","displayName":"Output","values":[{"displayName":"Type","name":"type","type":"options","options":[{"name":"Chain","value":"ai_chain"},{"name":"Document","value":"ai_document"},{"name":"Embedding","value":"ai_embedding"},{"name":"Language Model","value":"ai_languageModel"},{"name":"Memory","value":"ai_memory"},{"name":"Output Parser","value":"ai_outputParser"},{"name":"Text Splitter","value":"ai_textSplitter"},{"name":"Tool","value":"ai_tool"},{"name":"Vector Store","value":"ai_vectorStore"},{"name":"Main","value":"main"}],"noDataExpression":true,"default":"","required":true,"description":"The type of the input"}]}]}]},
{"displayName":"Binary Input Loader","name":"@n8n/n8n-nodes-langchain.documentBinaryInputLoader","group":["transform"],"version":1,"description":"Use binary data from a previous step in the workflow","defaults":{"name":"Binary Input Loader"},"codex":{"categories":["AI"],"subcategories":{"AI":["Document Loaders"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentbinaryinputloader/"}]}},"inputs":[{"displayName":"Text Splitter","maxConnections":1,"type":"ai_textSplitter","required":true}],"outputs":["ai_document"],"outputNames":["Document"],"properties":[{"displayName":"Loader Type","name":"loader","type":"options","default":"jsonLoader","required":true,"options":[{"name":"CSV Loader","value":"csvLoader","description":"Load CSV files"},{"name":"Docx Loader","value":"docxLoader","description":"Load Docx documents"},{"name":"EPub Loader","value":"epubLoader","description":"Load EPub files"},{"name":"JSON Loader","value":"jsonLoader","description":"Load JSON files"},{"name":"PDF Loader","value":"pdfLoader","description":"Load PDF documents"},{"name":"Text Loader","value":"textLoader","description":"Load plain text files"}]},{"displayName":"Binary Data Key","name":"binaryDataKey","type":"string","default":"data","required":true,"description":"Name of the binary property from which to read the file buffer"},{"displayName":"Split Pages","name":"splitPages","type":"boolean","default":true,"displayOptions":{"show":{"loader":["pdfLoader"]}}},{"displayName":"Column","name":"column","type":"string","default":"","description":"Column to extract from CSV","displayOptions":{"show":{"loader":["csvLoader"]}}},{"displayName":"Separator","name":"separator","type":"string","description":"Separator to use for CSV","default":",","displayOptions":{"show":{"loader":["csvLoader"]}}},{"displayName":"Pointers","name":"pointers","type":"string","default":"","description":"Pointers to extract from JSON, e.g. \"/text\" or \"/text, /meta/title\"","displayOptions":{"show":{"loader":["jsonLoader"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata","name":"metadata","type":"fixedCollection","description":"Metadata to add to each document. Could be used for filtering during retrieval","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add property","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/document_loaders/DocumentBinaryInputLoader/binary.svg"},
{"displayName":"GitHub Document Loader","name":"@n8n/n8n-nodes-langchain.documentGithubLoader","group":["transform"],"version":1,"description":"Use GitHub data as input to this chain","defaults":{"name":"GitHub Document Loader"},"codex":{"categories":["AI"],"subcategories":{"AI":["Document Loaders"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentgithubloader/"}]}},"credentials":[{"name":"githubApi","required":true}],"inputs":[{"displayName":"Text Splitter","maxConnections":1,"type":"ai_textSplitter"}],"inputNames":["Text Splitter"],"outputs":["ai_document"],"outputNames":["Document"],"properties":[{"displayName":"Repository Link","name":"repository","type":"string","default":""},{"displayName":"Branch","name":"branch","type":"string","default":"main"},{"displayName":"Options","name":"additionalOptions","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Recursive","name":"recursive","type":"boolean","default":false},{"displayName":"Ignore Paths","name":"recursive","type":"string","description":"Comma-separated list of paths to ignore, e.g. \"docs, src/tests","default":""}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/document_loaders/DocumentGithubLoader/github.svg"},
{"displayName":"JSON Input Loader","name":"@n8n/n8n-nodes-langchain.documentJsonInputLoader","group":["transform"],"version":1,"description":"Use JSON data from a previous step in the workflow","defaults":{"name":"JSON Input Loader"},"codex":{"categories":["AI"],"subcategories":{"AI":["Document Loaders"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentjsoninputloader/"}]}},"inputs":[{"displayName":"Text Splitter","maxConnections":1,"type":"ai_textSplitter"}],"inputNames":["Text Splitter"],"outputs":["ai_document"],"outputNames":["Document"],"properties":[{"displayName":"Pointers","name":"pointers","type":"string","default":"","description":"Pointers to extract from JSON, e.g. \"/text\" or \"/text, /meta/title\""},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata","name":"metadata","type":"fixedCollection","description":"Metadata to add to each document. Could be used for filtering during retrieval","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add property","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/document_loaders/DocumentJSONInputLoader/json.svg"},
{"displayName":"Embeddings Cohere","name":"@n8n/n8n-nodes-langchain.embeddingsCohere","group":["transform"],"version":1,"description":"Use Cohere Embeddings","defaults":{"name":"Embeddings Cohere"},"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"credentials":[{"name":"cohereApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingscohere/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.","name":"notice","type":"notice","default":""},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the embeddings. <a href=\"https://docs.cohere.com/docs/models\">Learn more</a>.","default":"embed-english-v2.0","options":[{"name":"Embed-English-v2.0(4096 Dimensions)","value":"embed-english-v2.0"},{"name":"Embed-English-Light-v2.0(1024 Dimensions)","value":"embed-english-light-v2.0"},{"name":"Embed-Multilingual-v2.0(768 Dimensions)","value":"embed-multilingual-v2.0"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsCohere/cohere.svg"},
{"displayName":"Embeddings AWS Bedrock","name":"@n8n/n8n-nodes-langchain.embeddingsAwsBedrock","credentials":[{"name":"aws","required":true}],"group":["transform"],"version":1,"description":"Use Embeddings AWS Bedrock","defaults":{"name":"Embeddings AWS Bedrock"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsawsbedrock/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"=https://bedrock.{{$credentials?.region ?? \"eu-central-1\"}}.amazonaws.com"},"properties":[{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/foundation-models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"modelSummaries"}},{"type":"filter","properties":{"pass":"={{ !'anthropic.claude-instant-v1-100k,anthropic.claude-v2,amazon.titan-text-express-v1'.match($responseItem.modelId) }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.modelName}}","description":"={{$responseItem.modelArn}}","value":"={{$responseItem.modelId}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":""}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsAwsBedrock/bedrock.svg"},
{"displayName":"Embeddings Google PaLM","name":"@n8n/n8n-nodes-langchain.embeddingsGooglePalm","group":["transform"],"version":1,"description":"Use Google PaLM Embeddings","defaults":{"name":"Embeddings Google PaLM"},"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"credentials":[{"name":"googlePalmApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsgooglepalm/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.","name":"notice","type":"notice","default":""},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the embeddings. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta3/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ $responseItem.name.startsWith('models/embedding') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"models/embedding-gecko-001"}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsGooglePalm/google.svg"},
{"displayName":"Embeddings Hugging Face Inference","name":"@n8n/n8n-nodes-langchain.embeddingsHuggingFaceInference","group":["transform"],"version":1,"description":"Use HuggingFace Inference Embeddings","defaults":{"name":"Embeddings HuggingFace Inference"},"credentials":[{"name":"huggingFaceApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingshuggingfaceinference/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.","name":"notice","type":"notice","default":""},{"displayName":"Model Name","name":"modelName","type":"string","default":"sentence-transformers/distilbert-base-nli-mean-tokens","description":"The model name to use from HuggingFace library"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Custom Inference Endpoint","name":"endpointUrl","default":"","description":"Custom endpoint URL","type":"string"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsHuggingFaceInference/huggingface.svg"},
{"displayName":"Embeddings OpenAI","name":"@n8n/n8n-nodes-langchain.embeddingsOpenAi","credentials":[{"name":"openAiApi","required":true}],"group":["transform"],"version":1,"description":"Use Embeddings OpenAI","defaults":{"name":"Embeddings OpenAI"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsopenai/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Batch Size","name":"batchSize","default":512,"typeOptions":{"maxValue":2048},"description":"Maximum number of documents to send in each request","type":"number"},{"displayName":"Strip New Lines","name":"stripNewLines","default":true,"description":"Whether to strip new lines from the input text","type":"boolean"},{"displayName":"Timeout","name":"timeout","default":-1,"description":"Maximum amount of time a request is allowed to take in seconds. Set to -1 for no timeout.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsOpenAI/openAi.svg"},
{"displayName":"Embeddings TensorFlow","name":"@n8n/n8n-nodes-langchain.embeddingsTensorFlow","group":["transform"],"version":1,"description":"Use Embeddings TensorFlow","defaults":{"name":"Embeddings TensorFlow"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingstensorflow/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"The TensorFlow model we use for generating embeddings is using 512-dimensional embeddings. Please make sure to use the same dimensionality for your vector store. Be aware that running this model with high-dimensional embeddings may result in high CPU usage on the machine.","name":"notice","type":"notice","default":""}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsTensorFlow/tensorflow.svg"},
{"displayName":"Anthropic Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatAnthropic","group":["transform"],"version":1,"description":"Language Model Anthropic","defaults":{"name":"Anthropic Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatanthropic/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"anthropicApi","required":true}],"properties":[{"displayName":"Model","name":"model","type":"options","options":[{"name":"Claude","value":"claude-2"},{"name":"Claude Instant","value":"claude-instant-1"}],"description":"The model which will generate the completion. <a href=\"https://docs.anthropic.com/claude/reference/selecting-a-model\">Learn more</a>.","default":"claude-2"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokensToSample","default":32768,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":-1,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMChatAnthropic/anthropic.svg"},
{"displayName":"Google PaLM Language Model","name":"@n8n/n8n-nodes-langchain.lmGooglePalm","group":["transform"],"version":1,"description":"Language Model Google PaLM","defaults":{"name":"Google PaLM Language Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmgooglepalm/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"googlePalmApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"properties":[{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the completion. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta3/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ $responseItem.name.startsWith('models/text') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"models/text-bison-001"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxOutputTokens","default":1024,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":40,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":0.9,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmGooglePalm/google.svg"},
{"displayName":"AWS Bedrock Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatAwsBedrock","group":["transform"],"version":1,"description":"Language Model AWS Bedrock","defaults":{"name":"AWS Bedrock Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatawsbedrock/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"aws","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"=https://bedrock.{{$credentials?.region ?? \"eu-central-1\"}}.amazonaws.com"},"properties":[{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/foundation-models?&byOutputModality=TEXT"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"modelSummaries"}},{"type":"filter","properties":{"pass":"={{ !['anthropic.claude-instant-v1-100k'].includes($responseItem.modelId) }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.modelName}}","description":"={{$responseItem.modelArn}}","value":"={{$responseItem.modelId}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":""},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokensToSample","default":2000,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatAwsBedrock/bedrock.svg"},
{"displayName":"Google PaLM Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatGooglePalm","group":["transform"],"version":1,"description":"Chat Model Google PaLM","defaults":{"name":"Google PaLM Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglepalm/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"googlePalmApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"properties":[{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the completion. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta3/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ $responseItem.name.startsWith('models/chat') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"models/chat-bison-001"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":40,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":0.9,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatGooglePalm/google.svg"},
{"displayName":"Ollama Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatOllama","group":["transform"],"version":1,"description":"Language Model Ollama","defaults":{"name":"Ollama Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"ollamaApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.baseUrl.replace(new RegExp(\"/$\"), \"\") }}"},"properties":[{"displayName":"Model","name":"model","type":"options","default":"llama2","description":"The model which will generate the completion. To download models, visit <a href=\"https://ollama.ai/library\">Ollama Models Library</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/api/tags"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"required":true},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":-1,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMChatOllama/ollama.svg"},
{"displayName":"OpenAI Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatOpenAi","group":["transform"],"version":1,"description":"For advanced usage with an AI chain","defaults":{"name":"OpenAI Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"openAiApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $parameter.options?.baseURL?.split(\"/\").slice(0,-1).join(\"/\") || \"https://api.openai.com\" }}"},"properties":[{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://beta.openai.com/docs/models/overview\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"={{ $parameter.options?.baseURL?.split(\"/\").slice(-1).pop() || \"v1\"  }}/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ $responseItem.id.startsWith('gpt-') && !$responseItem.id.includes('instruct') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.id}}","value":"={{$responseItem.id}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"gpt-3.5-turbo"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":-1,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Presence Penalty","name":"presencePenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Timeout","name":"timeout","default":0,"description":"Maximum amount of time a request is allowed to take in seconds","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMChatOpenAi/openAi.svg"},
{"displayName":"OpenAI Model","name":"@n8n/n8n-nodes-langchain.lmOpenAi","group":["transform"],"version":1,"description":"For advanced usage with an AI chain","defaults":{"name":"OpenAI Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmopenai/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"openAiApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $parameter.options?.baseURL?.split(\"/\").slice(0,-1).join(\"/\") || \"https://api.openai.com\" }}"},"properties":[{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://beta.openai.com/docs/models/overview\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"={{ $parameter.options?.baseURL?.split(\"/\").slice(-1).pop() || \"v1\"  }}/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{$responseItem.owned_by.startsWith('system')"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.id}}","value":"={{$responseItem.id}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"gpt-3.5-turbo-instruct"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":-1,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Presence Penalty","name":"presencePenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Timeout","name":"timeout","default":0,"description":"Maximum amount of time a request is allowed to take in seconds","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMOpenAi/openAi.svg"},
{"displayName":"Cohere Model","name":"@n8n/n8n-nodes-langchain.lmCohere","group":["transform"],"version":1,"description":"Language Model Cohere","defaults":{"name":"Cohere Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmcohere/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"cohereApi","required":true}],"properties":[{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":250,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Model","name":"model","type":"string","description":"The name of the model to use","default":""},{"displayName":"Sampling Temperature","name":"temperature","default":0,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMCohere/cohere.svg"},
{"displayName":"Ollama Model","name":"@n8n/n8n-nodes-langchain.lmOllama","group":["transform"],"version":1,"description":"Language Model Ollama","defaults":{"name":"Ollama Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"ollamaApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.baseUrl.replace(new RegExp(\"/$\"), \"\") }}"},"properties":[{"displayName":"Model","name":"model","type":"options","default":"llama2","description":"The model which will generate the completion. To download models, visit <a href=\"https://ollama.ai/library\">Ollama Models Library</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/api/tags"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"required":true},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":-1,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMOllama/ollama.svg"},
{"displayName":"Hugging Face Inference Model","name":"@n8n/n8n-nodes-langchain.lmOpenHuggingFaceInference","group":["transform"],"version":1,"description":"Language Model HuggingFaceInference","defaults":{"name":"Hugging Face Inference Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmopenhuggingfaceinference/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"huggingFaceApi","required":true}],"properties":[{"displayName":"Model","name":"model","type":"string","default":"gpt2"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Custom Inference Endpoint","name":"endpointUrl","default":"","description":"Custom endpoint URL","type":"string"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":128,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Presence Penalty","name":"presencePenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls the top tokens to consider within the sample operation to create new text","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMOpenHuggingFaceInference/huggingface.svg"},
{"displayName":"Window Buffer Memory","name":"@n8n/n8n-nodes-langchain.memoryBufferWindow","icon":"fa:database","group":["transform"],"version":1,"description":"Stores the chat history in a windowed buffer. Refreshes on restart.","defaults":{"name":"Window Buffer Memory"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"properties":[{"displayName":"Session Key","name":"sessionKey","type":"string","default":"chat_history","description":"The key to use to store the memory in the workflow data"},{"displayName":"Context Window Length","name":"contextWindowLength","type":"number","default":5,"description":"The number of previous messages to consider for context"}]},
{"displayName":"Motorhead","name":"@n8n/n8n-nodes-langchain.memoryMotorhead","icon":"fa:file-export","group":["transform"],"version":1,"description":"Use Motorhead Memory","defaults":{"name":"Motorhead"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymotorhead/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"credentials":[{"name":"motorheadApi","required":true}],"properties":[{"displayName":"Session ID","name":"sessionId","type":"string","required":true,"default":""}]},
{"displayName":"Redis Chat Memory","name":"@n8n/n8n-nodes-langchain.memoryRedisChat","group":["transform"],"version":1,"description":"Stores the chat history in Redis.","defaults":{"name":"Redis Chat Memory"},"credentials":[{"name":"redis","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryredischat/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"properties":[{"displayName":"Session Key","name":"sessionKey","type":"string","default":"chat_history","description":"The key to use to store the memory in the workflow data"},{"displayName":"Session Time To Live","name":"sessionTTL","type":"number","default":0,"description":"For how long the session should be stored in seconds. If set to 0 it will not expire."}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/memory/MemoryRedisChat/redis.svg"},
{"displayName":"Chat Messages Retriever","name":"@n8n/n8n-nodes-langchain.memoryChatRetriever","icon":"fa:database","group":["transform"],"version":1,"description":"Retrieve chat messages from memory and use them in the workflow","defaults":{"name":"Chat Messages Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Miscellaneous"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.memorychatretriever/"}]}},"inputs":["main",{"displayName":"Memory","maxConnections":1,"type":"ai_memory","required":true}],"outputs":["main"],"properties":[{"displayName":"Simplify Output","name":"simplifyOutput","type":"boolean","description":"Whether to simplify the output to only include the sender and the text","default":true}]},
{"displayName":"Xata","name":"@n8n/n8n-nodes-langchain.memoryXata","group":["transform"],"version":1,"description":"Use Xata Memory","defaults":{"name":"Xata","color":"#1321A7"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryxata/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"credentials":[{"name":"xataApi","required":true}],"properties":[{"displayName":"Session ID","name":"sessionId","type":"string","required":true,"default":""}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/memory/MemoryXata/xata.svg"},
{"displayName":"Zep","name":"@n8n/n8n-nodes-langchain.memoryZep","group":["transform"],"version":1,"description":"Use Zep Memory","defaults":{"name":"Zep"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryzep/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"credentials":[{"name":"zepApi","required":true}],"properties":[{"displayName":"Session ID","name":"sessionId","type":"string","required":true,"default":""}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/memory/MemoryZep/zep.png"},
{"displayName":"Auto-fixing Output Parser","name":"@n8n/n8n-nodes-langchain.outputParserAutofixing","icon":"fa:tools","group":["transform"],"version":1,"description":"Automatically fix the output if it is not in the correct format","defaults":{"name":"Auto-fixing Output Parser"},"codex":{"categories":["AI"],"subcategories":{"AI":["Output Parsers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserautofixing/"}]}},"inputs":[{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Output Parser","maxConnections":1,"required":true,"type":"ai_outputParser"}],"outputs":["ai_outputParser"],"outputNames":["Output Parser"],"properties":[]},
{"displayName":"Item List Output Parser","name":"@n8n/n8n-nodes-langchain.outputParserItemList","icon":"fa:bars","group":["transform"],"version":1,"description":"Return the results as separate items","defaults":{"name":"Item List Output Parser"},"codex":{"categories":["AI"],"subcategories":{"AI":["Output Parsers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparseritemlist/"}]}},"inputs":[],"outputs":["ai_outputParser"],"outputNames":["Output Parser"],"properties":[{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Number Of Items","name":"numberOfItems","type":"number","default":-1,"description":"Defines many many items should be returned maximally. If set to -1, there is no limit."},{"displayName":"Separator","name":"separator","type":"string","default":"\\n","description":"Defines the separator that should be used to split the results into separate items. Defaults to a new line but can be changed depending on the data that should be returned."}]}]},
{"displayName":"Structured Output Parser","name":"@n8n/n8n-nodes-langchain.outputParserStructured","icon":"fa:code","group":["transform"],"version":1,"description":"Return data in a defined JSON format","defaults":{"name":"Structured Output Parser"},"codex":{"alias":["json","zod"],"categories":["AI"],"subcategories":{"AI":["Output Parsers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/"}]}},"inputs":[],"outputs":["ai_outputParser"],"outputNames":["Output Parser"],"properties":[{"displayName":"JSON Schema","name":"jsonSchema","type":"json","description":"JSON Schema to structure and validate the output against","default":"{\n  \"type\": \"object\",\n  \"properties\": {\n    \"state\": {\n      \"type\": \"string\"\n    },\n    \"cities\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      }\n    }\n  }\n}","typeOptions":{"rows":10,"editor":"json","editorLanguage":"json"},"required":true},{"displayName":"The schema has to be defined in the <a target=\"_blank\" href=\"https://json-schema.org/\">JSON Schema</a> format. Look at <a target=\"_blank\" href=\"https://json-schema.org/learn/miscellaneous-examples.html\">this</a> page for examples.","name":"notice","type":"notice","default":""}]},
{"displayName":"Contextual Compression Retriever","name":"@n8n/n8n-nodes-langchain.retrieverContextualCompression","icon":"fa:box-open","group":["transform"],"version":1,"description":"Enhances document similarity search by contextual compression.","defaults":{"name":"Contextual Compression Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievercontextualcompression/"}]}},"inputs":[{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever","required":true}],"outputs":[{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever"}],"properties":[]},
{"displayName":"Vector Store Retriever","name":"@n8n/n8n-nodes-langchain.retrieverVectorStore","icon":"fa:box-open","group":["transform"],"version":1,"description":"Use a Vector Store as Retriever","defaults":{"name":"Vector Store Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore/"}]}},"inputs":[{"displayName":"Vector Store","maxConnections":1,"type":"ai_vectorStore","required":true}],"outputs":["ai_retriever"],"outputNames":["Retriever"],"properties":[{"displayName":"Top K","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store"}]},
{"displayName":"MultiQuery Retriever","name":"@n8n/n8n-nodes-langchain.retrieverMultiQuery","icon":"fa:box-open","group":["transform"],"version":1,"description":"Automates prompt tuning, generates diverse queries and expands document pool for enhanced retrieval.","defaults":{"name":"MultiQuery Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievermultiquery/"}]}},"inputs":[{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever","required":true}],"outputs":[{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever"}],"properties":[{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Query Count","name":"queryCount","default":3,"typeOptions":{"minValue":1},"description":"Number of different versions of the given question to generate","type":"number"}]}]},
{"displayName":"Workflow Retriever","name":"@n8n/n8n-nodes-langchain.retrieverWorkflow","icon":"fa:box-open","group":["transform"],"version":1,"description":"Use an n8n Workflow as Retriever","defaults":{"name":"Workflow Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrieverworkflow/"}]}},"inputs":[],"outputs":[{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever"}],"properties":[{"displayName":"The workflow will receive \"query\" as input and the output of the last node will be returned and converted to Documents","name":"executeNotice","type":"notice","default":""},{"displayName":"Source","name":"source","type":"options","options":[{"name":"Database","value":"database","description":"Load the workflow from the database by ID"},{"name":"Parameter","value":"parameter","description":"Load the workflow from a parameter"}],"default":"database","description":"Where to get the workflow to execute from"},{"displayName":"Workflow ID","name":"workflowId","type":"string","displayOptions":{"show":{"source":["database"]}},"default":"","required":true,"description":"The workflow to execute"},{"displayName":"Workflow JSON","name":"workflowJson","type":"string","typeOptions":{"editor":"json","rows":10},"displayOptions":{"show":{"source":["parameter"]}},"default":"\n\n\n","required":true,"description":"The workflow JSON code to execute"},{"displayName":"Workflow Values","name":"fields","placeholder":"Add Value","type":"fixedCollection","description":"Set the values which should be made available in the workflow","typeOptions":{"multipleValues":true,"sortable":true},"default":{},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"e.g. fieldName","description":"Name of the field to set the value of. Supports dot-notation. Example: data.person[0].name.","requiresDataPath":"single"},{"displayName":"Type","name":"type","type":"options","description":"The field value type","options":[{"name":"String","value":"stringValue"},{"name":"Number","value":"numberValue"},{"name":"Boolean","value":"booleanValue"},{"name":"Array","value":"arrayValue"},{"name":"Object","value":"objectValue"}],"default":"stringValue"},{"displayName":"Value","name":"stringValue","type":"string","default":"","displayOptions":{"show":{"type":["stringValue"]}},"validateType":"string","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"numberValue","type":"string","default":"","displayOptions":{"show":{"type":["numberValue"]}},"validateType":"number","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"booleanValue","type":"options","default":"true","options":[{"name":"True","value":"true"},{"name":"False","value":"false"}],"displayOptions":{"show":{"type":["booleanValue"]}},"validateType":"boolean","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"arrayValue","type":"string","default":"","placeholder":"e.g. [ arrayItem1, arrayItem2, arrayItem3 ]","displayOptions":{"show":{"type":["arrayValue"]}},"validateType":"array","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"objectValue","type":"string","default":"={}","typeOptions":{"editor":"json","editorLanguage":"json","rows":2},"displayOptions":{"show":{"type":["objectValue"]}},"validateType":"object","ignoreValidationDuringExecution":true}]}]}]},
{"displayName":"Character Text Splitter","name":"@n8n/n8n-nodes-langchain.textSplitterCharacterTextSplitter","icon":"fa:grip-lines-vertical","group":["transform"],"version":1,"description":"Split text into chunks by characters","defaults":{"name":"Character Text Splitter"},"codex":{"categories":["AI"],"subcategories":{"AI":["Text Splitters"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplittercharactertextsplitter/"}]}},"inputs":[],"outputs":["ai_textSplitter"],"outputNames":["Text Splitter"],"properties":[{"displayName":"Separator","name":"separator","type":"string","default":""},{"displayName":"Chunk Size","name":"chunkSize","type":"number","default":1000},{"displayName":"Chunk Overlap","name":"chunkOverlap","type":"number","default":0}]},
{"displayName":"Recursive Character Text Splitter","name":"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter","icon":"fa:grip-lines-vertical","group":["transform"],"version":1,"description":"Split text into chunks by characters recursively, recommended for most use cases","defaults":{"name":"Recursive Character Text Splitter"},"codex":{"categories":["AI"],"subcategories":{"AI":["Text Splitters"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplitterrecursivecharactertextsplitter/"}]}},"inputs":[],"outputs":["ai_textSplitter"],"outputNames":["Text Splitter"],"properties":[{"displayName":"Chunk Size","name":"chunkSize","type":"number","default":1000},{"displayName":"Chunk Overlap","name":"chunkOverlap","type":"number","default":0}]},
{"displayName":"Token Splitter","name":"@n8n/n8n-nodes-langchain.textSplitterTokenSplitter","icon":"fa:grip-lines-vertical","group":["transform"],"version":1,"description":"Split text into chunks by tokens","defaults":{"name":"Token Splitter"},"codex":{"categories":["AI"],"subcategories":{"AI":["Text Splitters"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplittertokensplitter/"}]}},"inputs":[],"outputs":["ai_textSplitter"],"outputNames":["Text Splitter"],"properties":[{"displayName":"Chunk Size","name":"chunkSize","type":"number","default":1000},{"displayName":"Chunk Overlap","name":"chunkOverlap","type":"number","default":0}]},
{"displayName":"Calculator","name":"@n8n/n8n-nodes-langchain.toolCalculator","icon":"fa:calculator","group":["transform"],"version":1,"description":"Use Calculator","defaults":{"name":"Calculator"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcalculator/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[]},
{"displayName":"Code Tool","name":"@n8n/n8n-nodes-langchain.toolCode","icon":"fa:code","group":["transform"],"version":1,"description":"Create a tool via code","defaults":{"name":"Code Tool"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcode/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"My_Tool"},{"displayName":"Description","name":"description","type":"string","default":"","placeholder":"Call this tool to get a random color. The input should be a string with comma separted names of colors to exclude.","typeOptions":{"rows":3}},{"displayName":"Language","name":"language","type":"options","noDataExpression":true,"options":[{"name":"JavaScript","value":"javaScript"},{"name":"Python (Beta)","value":"python"}],"default":"javaScript"},{"displayName":"JavaScript","name":"jsCode","type":"string","displayOptions":{"show":{"language":["javaScript"]}},"typeOptions":{"editor":"codeNodeEditor","editorLanguage":"javaScript"},"default":"","hint":"You can access the input the tool receives via the input property \"query\". The returned value should be a single string.","description":"JavaScript code to execute.<br><br>Tip: You can use luxon vars like <code>$today</code> for dates and <code>$jmespath</code> for querying JSON structures. <a href=\"https://docs.n8n.io/nodes/n8n-nodes-base.function\">Learn more</a>.","noDataExpression":true},{"displayName":"Python","name":"pythonCode","type":"string","displayOptions":{"show":{"language":["python"]}},"typeOptions":{"editor":"codeNodeEditor","editorLanguage":"python"},"default":"","hint":"You can access the input the tool receives via the input property \"query\". The returned value should be a single string.","description":"Python code to execute.<br><br>Tip: You can use luxon vars like <code>_today</code> for dates and <code>$_mespath</code> for querying JSON structures. <a href=\"https://docs.n8n.io/nodes/n8n-nodes-base.function\">Learn more</a>.","noDataExpression":true}]},
{"displayName":"SerpAPI","name":"@n8n/n8n-nodes-langchain.toolSerpApi","group":["transform"],"version":1,"description":"Search in Google using SerpAPI","defaults":{"name":"SerpAPI"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolserpapi/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"credentials":[{"name":"serpApi","required":true}],"properties":[{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Country","name":"gl","type":"string","default":"us","description":"Defines the country to use for search. Head to <a href=\"https://serpapi.com/google-countries\">Google countries page</a> for a full list of supported countries."},{"displayName":"Device","name":"device","type":"options","options":[{"name":"Desktop","value":"desktop"},{"name":"Mobile","value":"mobile"},{"name":"Tablet","value":"tablet"}],"default":"desktop","description":"Device to use to get the results"},{"displayName":"Explicit Array","name":"no_cache","type":"boolean","default":false,"description":"Whether to force SerpApi to fetch the Google results even if a cached version is already present. Cache expires after 1h. Cached searches are free, and are not counted towards your searches per month."},{"displayName":"Google Domain","name":"google_domain","type":"string","default":"google.com","description":"Defines the country to use for search. Head to <a href=\"https://serpapi.com/google-countries\">Google countries page</a> for a full list of supported countries."},{"displayName":"Language","name":"hl","type":"string","default":"en","description":"Defines the language to use. It's a two-letter language code. (e.g., `en` for English, `es` for Spanish, or `fr` for French). Head to <a href=\"https://serpapi.com/google-languages\">Google languages page</a> for a full list of supported languages."}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolSerpApi/serpApi.svg"},
{"displayName":"Wikipedia","name":"@n8n/n8n-nodes-langchain.toolWikipedia","group":["transform"],"version":1,"description":"Search in Wikipedia","defaults":{"name":"Wikipedia"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolwikipedia/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolWikipedia/wikipedia.svg"},
{"displayName":"Wolfram|Alpha","name":"@n8n/n8n-nodes-langchain.toolWolframAlpha","group":["transform"],"version":1,"description":"Connects to WolframAlpha's computational intelligence engine.","defaults":{"name":"Wolfram Alpha"},"credentials":[{"name":"wolframAlphaApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolwolframalpha/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolWolframAlpha/wolfram-alpha.svg"},
{"displayName":"Workflow Tool","name":"@n8n/n8n-nodes-langchain.toolWorkflow","icon":"fa:network-wired","group":["transform"],"version":1,"description":"Create a tool via a workflow","defaults":{"name":"Workflow Tool"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolworkflow/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"My_Color_Tool"},{"displayName":"Description","name":"description","type":"string","default":"","placeholder":"Call this tool to get a random color. The input should be a string with comma separted names of colors to exclude.","typeOptions":{"rows":3}},{"displayName":"The workflow will receive \"query\" as input and the output of the last node will be returned as response","name":"executeNotice","type":"notice","default":""},{"displayName":"Source","name":"source","type":"options","options":[{"name":"Database","value":"database","description":"Load the workflow from the database by ID"},{"name":"Parameter","value":"parameter","description":"Load the workflow from a parameter"}],"default":"database","description":"Where to get the workflow to execute from"},{"displayName":"Workflow ID","name":"workflowId","type":"string","displayOptions":{"show":{"source":["database"]}},"default":"","required":true,"description":"The workflow to execute"},{"displayName":"Workflow JSON","name":"workflowJson","type":"string","typeOptions":{"editor":"json","rows":10},"displayOptions":{"show":{"source":["parameter"]}},"default":"\n\n\n","required":true,"description":"The workflow JSON code to execute"},{"displayName":"Response Property Name","name":"responsePropertyName","type":"string","default":"response","description":"The name of the property of the last node that will be returned as response"},{"displayName":"Workflow Values","name":"fields","placeholder":"Add Value","type":"fixedCollection","description":"Set the values which should be made available in the workflow","typeOptions":{"multipleValues":true,"sortable":true},"default":{},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"e.g. fieldName","description":"Name of the field to set the value of. Supports dot-notation. Example: data.person[0].name.","requiresDataPath":"single"},{"displayName":"Type","name":"type","type":"options","description":"The field value type","options":[{"name":"String","value":"stringValue"},{"name":"Number","value":"numberValue"},{"name":"Boolean","value":"booleanValue"},{"name":"Array","value":"arrayValue"},{"name":"Object","value":"objectValue"}],"default":"stringValue"},{"displayName":"Value","name":"stringValue","type":"string","default":"","displayOptions":{"show":{"type":["stringValue"]}},"validateType":"string","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"numberValue","type":"string","default":"","displayOptions":{"show":{"type":["numberValue"]}},"validateType":"number","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"booleanValue","type":"options","default":"true","options":[{"name":"True","value":"true"},{"name":"False","value":"false"}],"displayOptions":{"show":{"type":["booleanValue"]}},"validateType":"boolean","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"arrayValue","type":"string","default":"","placeholder":"e.g. [ arrayItem1, arrayItem2, arrayItem3 ]","displayOptions":{"show":{"type":["arrayValue"]}},"validateType":"array","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"objectValue","type":"string","default":"={}","typeOptions":{"editor":"json","editorLanguage":"json","rows":2},"displayOptions":{"show":{"type":["objectValue"]}},"validateType":"object","ignoreValidationDuringExecution":true}]}]}]},
{"displayName":"Manual Chat Trigger","name":"@n8n/n8n-nodes-langchain.manualChatTrigger","icon":"fa:comments","group":["trigger"],"version":1,"description":"Runs the flow on new manual chat message","eventTriggerDescription":"","maxNodes":1,"defaults":{"name":"On new manual Chat Message","color":"#909298"},"codex":{"categories":["Core Nodes"],"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.manualchattrigger/"}]},"subcategories":{"Core Nodes":["Other Trigger Nodes"]}},"inputs":[],"outputs":["main"],"properties":[{"displayName":"This node is where a manual chat workflow execution starts. To make one, go back to the canvas and click ‘Chat’","name":"notice","type":"notice","default":""},{"displayName":"Chat and execute workflow","name":"openChat","type":"button","typeOptions":{"action":"openChat"},"default":""}]},
{"displayName":"In-Memory Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStoreInMemory","description":"Work with your data in In-Memory Vector Store","icon":"fa:database","group":["transform"],"version":1,"defaults":{"name":"In-Memory Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/"}]}},"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"Memory Key","name":"memoryKey","type":"string","default":"vector_store_key","description":"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions."},{"displayName":"The embbded data are stored in the server memory, so they will be lost when the server is restarted. Additionally, if the amount of data is too large, it may cause the server to crash due to insufficient memory.","name":"notice","type":"notice","default":"","displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Clear Store","name":"clearStore","type":"boolean","default":false,"description":"Whether to clear the store before inserting new data","displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}}]},
{"displayName":"In Memory Vector Store Insert","name":"@n8n/n8n-nodes-langchain.vectorStoreInMemoryInsert","icon":"fa:database","group":["transform"],"version":1,"hidden":true,"description":"Insert data into an in-memory vector store","defaults":{"name":"In Memory Vector Store Insert"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemoryinsert/"}]}},"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"The embbded data are stored in the server memory, so they will be lost when the server is restarted. Additionally, if the amount of data is too large, it may cause the server to crash due to insufficient memory.","name":"notice","type":"notice","default":""},{"displayName":"Clear Store","name":"clearStore","type":"boolean","default":false,"description":"Whether to clear the store before inserting new data"},{"displayName":"Memory Key","name":"memoryKey","type":"string","default":"vector_store_key","description":"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions."}]},
{"displayName":"In Memory Vector Store Load","name":"@n8n/n8n-nodes-langchain.vectorStoreInMemoryLoad","icon":"fa:database","group":["transform"],"version":1,"hidden":true,"description":"Load embedded data from an in-memory vector store","defaults":{"name":"In Memory Vector Store Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.vectorstoreinmemoryload/"}]}},"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Memory Key","name":"memoryKey","type":"string","default":"vector_store_key","description":"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions."}]},
{"displayName":"Pinecone Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStorePinecone","description":"Work with your data in Pinecone Vector Store","group":["transform"],"version":1,"defaults":{"name":"Pinecone Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/"}]}},"credentials":[{"name":"pineconeApi","required":true}],"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"Pinecone Index","name":"pineconeIndex","type":"string","default":"","required":true},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Clear Namespace","name":"clearNamespace","type":"boolean","default":false,"description":"Whether to clear the namespace before inserting new data"},{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","description":"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.","default":""}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","description":"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.","default":""},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","description":"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.","default":""},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["retrieve"]}}}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStorePinecone/pinecone.svg"},
{"displayName":"Pinecone: Insert","hidden":true,"name":"@n8n/n8n-nodes-langchain.vectorStorePineconeInsert","group":["transform"],"version":1,"description":"Insert data into Pinecone Vector Store index","defaults":{"name":"Pinecone: Insert","color":"#1321A7"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepineconeinsert/"}]}},"credentials":[{"name":"pineconeApi","required":true}],"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"Pinecone Index","name":"pineconeIndex","type":"string","default":"","required":true},{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","default":""},{"displayName":"Specify the document to load in the document loader sub-node","name":"notice","type":"notice","default":""},{"displayName":"Clear Namespace","name":"clearNamespace","type":"boolean","default":false,"description":"Whether to clear the namespace before inserting new data"}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStorePineconeInsert/pinecone.svg"},
{"displayName":"Pinecone: Load","hidden":true,"name":"@n8n/n8n-nodes-langchain.vectorStorePineconeLoad","group":["transform"],"version":1,"description":"Load data from Pinecone Vector Store index","defaults":{"name":"Pinecone: Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.vectorstorepineconeload/"}]}},"credentials":[{"name":"pineconeApi","required":true}],"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Pinecone Index","name":"pineconeIndex","type":"string","default":"","required":true},{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","default":""},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStorePineconeLoad/pinecone.svg"},
{"displayName":"Supabase Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStoreSupabase","description":"Work with your data in Supabase Vector Store","group":["transform"],"version":1,"defaults":{"name":"Supabase Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoresupabase/"}]}},"credentials":[{"name":"supabaseApi","required":true}],"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"Table Name","name":"tableName","type":"string","default":"","required":true,"description":"Name of the table to load from"},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","description":"Name of the query to use for matching documents"}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","description":"Name of the query to use for matching documents"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","description":"Name of the query to use for matching documents"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["retrieve"]}}}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreSupabase/supabase.svg"},
{"displayName":"Supabase: Insert","hidden":true,"name":"@n8n/n8n-nodes-langchain.vectorStoreSupabaseInsert","group":["transform"],"version":1,"description":"Insert data into Supabase Vector Store index [https://supabase.com/docs/guides/ai/langchain]","defaults":{"name":"Supabase: Insert"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoresupabaseinsert/"}]}},"credentials":[{"name":"supabaseApi","required":true}],"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"Please refer to the <a href=\"https://supabase.com/docs/guides/ai/langchain\" target=\"_blank\">Supabase documentation</a> for more information on how to setup your database as a Vector Store.","name":"setupNotice","type":"notice","default":""},{"displayName":"Table Name","name":"tableName","type":"string","default":"","required":true,"description":"Name of the table to insert into"},{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","required":true,"description":"Name of the query to use for matching documents"},{"displayName":"Specify the document to load in the document loader sub-node","name":"notice","type":"notice","default":""}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreSupabaseInsert/supabase.svg"},
{"displayName":"Supabase: Load","name":"@n8n/n8n-nodes-langchain.vectorStoreSupabaseLoad","hidden":true,"group":["transform"],"version":1,"description":"Load data from Supabase Vector Store index","defaults":{"name":"Supabase: Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.vectorstoresupabaseload/"}]}},"credentials":[{"name":"supabaseApi","required":true}],"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Table Name","name":"tableName","type":"string","default":"","required":true,"description":"Name of the table to load from"},{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","required":true,"description":"Name of the query to use for matching documents"},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreSupabaseLoad/supabase.svg"},
{"displayName":"Zep Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStoreZep","description":"Work with your data in Zep Vector Store","group":["transform"],"version":1,"defaults":{"name":"Zep Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorezep/"}]}},"credentials":[{"name":"zepApi","required":true}],"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"Collection Name","name":"collectionName","type":"string","default":"","required":true},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Is Auto Embedded","name":"isAutoEmbedded","type":"boolean","default":true,"description":"Whether to automatically embed documents when they are added"}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["retrieve"]}}}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreZep/zep.png"},
{"displayName":"Zep Vector Store: Insert","name":"@n8n/n8n-nodes-langchain.vectorStoreZepInsert","hidden":true,"group":["transform"],"version":1,"description":"Insert data into Zep Vector Store index","defaults":{"name":"Zep: Insert"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorezepinsert/"}]}},"credentials":[{"name":"zepApi","required":true}],"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"Collection Name","name":"collectionName","type":"string","default":"","required":true},{"displayName":"Specify the document to load in the document loader sub-node","name":"notice","type":"notice","default":""},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Is Auto Embedded","name":"isAutoEmbedded","type":"boolean","default":true,"description":"Whether to automatically embed documents when they are added"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreZepInsert/zep.png"},
{"displayName":"Zep Vector Store: Load","name":"@n8n/n8n-nodes-langchain.vectorStoreZepLoad","hidden":true,"group":["transform"],"version":1,"description":"Load data from Zep Vector Store index","defaults":{"name":"Zep: Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.vectorstorezepload/"}]}},"credentials":[{"name":"zepApi","required":true}],"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Collection Name","name":"collectionName","type":"string","default":"","required":true},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreZepLoad/zep.png"}
]